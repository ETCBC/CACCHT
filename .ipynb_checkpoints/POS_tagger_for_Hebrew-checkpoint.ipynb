{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geitb\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF app is up-to-date.\n",
      "Using annotation/app-bhsa commit 43c1c5e88b371f575cdbbf57e38167deb8725f7f (=latest)\n",
      "  in C:\\Users\\geitb/text-fabric-data/__apps__/bhsa.\n",
      "Using etcbc/bhsa/tf - c r1.5 in C:\\Users\\geitb/text-fabric-data\n",
      "Using etcbc/phono/tf - c r1.2 in C:\\Users\\geitb/text-fabric-data\n",
      "Using etcbc/parallels/tf - c r1.2 in C:\\Users\\geitb/text-fabric-data\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Documentation:** <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa\" title=\"provenance of BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis\">BHSA</a> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Writing/Hebrew\" title=\"('Hebrew characters and transcriptions',)\">Character table</a> <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/0_home.html\" title=\"BHSA feature documentation\">Feature docs</a> <a target=\"_blank\" href=\"https://github.com/annotation/app-bhsa\" title=\"bhsa API documentation\">bhsa API</a> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/\" title=\"text-fabric-api\">Text-Fabric API 7.3.15</a> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Use/Search/\" title=\"Search Templates Introduction and Reference\">Search Reference</a>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<details open><summary><b>Loaded features</b>:</summary>\n",
       "<p><b>BHSA = Biblia Hebraica Stuttgartensia Amstelodamensis</b>: <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/book.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\book.tf\">book</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/book@ll.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\book@am.tf\">book@ll</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/chapter.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\chapter.tf\">chapter</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/code.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\code.tf\">code</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/det.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\det.tf\">det</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/freq_lex.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\freq_lex.tf\">freq_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/function.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\function.tf\">function</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/g_word.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\g_word.tf\">g_word</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/g_word_utf8.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\g_word_utf8.tf\">g_word_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/gloss.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\gloss.tf\">gloss</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/gn.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\gn.tf\">gn</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/label.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\label.tf\">label</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/language.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\language.tf\">language</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/lex.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\lex.tf\">lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/lex_utf8.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\lex_utf8.tf\">lex_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/ls.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\ls.tf\">ls</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/nametype.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\nametype.tf\">nametype</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/nu.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\nu.tf\">nu</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/number.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\number.tf\">number</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/otype.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\otype.tf\">otype</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/pdp.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\pdp.tf\">pdp</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/prs_gn.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\prs_gn.tf\">prs_gn</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/prs_nu.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\prs_nu.tf\">prs_nu</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/prs_ps.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\prs_ps.tf\">prs_ps</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/ps.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\ps.tf\">ps</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/qere.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\qere.tf\">qere</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/qere_trailer.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\qere_trailer.tf\">qere_trailer</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/qere_trailer_utf8.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\qere_trailer_utf8.tf\">qere_trailer_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/qere_utf8.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\qere_utf8.tf\">qere_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/rank_lex.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\rank_lex.tf\">rank_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/rela.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\rela.tf\">rela</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/sp.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\sp.tf\">sp</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/st.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\st.tf\">st</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/trailer.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\trailer.tf\">trailer</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/trailer_utf8.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\trailer_utf8.tf\">trailer_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/txt.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\txt.tf\">txt</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/typ.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\typ.tf\">typ</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/verse.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\verse.tf\">verse</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/voc_lex.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\voc_lex.tf\">voc_lex</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/voc_lex_utf8.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\voc_lex_utf8.tf\">voc_lex_utf8</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/vs.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\vs.tf\">vs</a>  <a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/vt.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\vt.tf\">vt</a>  <b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/mother.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\mother.tf\">mother</a></i></b>  <b><i><a target=\"_blank\" href=\"https://etcbc.github.io/bhsa/features/hebrew/c/oslots.html\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/bhsa/tf/c\\oslots.tf\">oslots</a></i></b> </p><p><b>Parallel Passages</b>: <b><i><a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/parallels/blob/master/programs/parallels.ipynb\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/parallels/tf/c\\crossref.tf\">crossref</a></i></b> </p><p><b>Phonetic Transcriptions</b>: <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/phono/tf/c\\phono.tf\">phono</a>  <a target=\"_blank\" href=\"https://nbviewer.jupyter.org/github/etcbc/phono/blob/master/programs/phono.ipynb\" title=\"C:\\Users\\geitb/text-fabric-data/etcbc/phono/tf/c\\phono_trailer.tf\">phono_trailer</a> </p></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.ttf?raw=true');\n",
       "  src: url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0a6611;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    direction: ltr;\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: x-small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1em 0em;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-style: italic;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".verse {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".vl {\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-end;\n",
       "    align-items: flex-end;\n",
       "    direction: ltr;\n",
       "    width: 100%;\n",
       "}\n",
       ".outeritem {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".sentence,.clause,.phrase {\n",
       "    margin-top: -1.2em;\n",
       "    margin-left: 1em;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3em;\n",
       "    border-style: solid;\n",
       "    border-radius: 0.2em;\n",
       "    font-size: small;\n",
       "    display: block;\n",
       "    width: fit-content;\n",
       "    max-width: fit-content;\n",
       "    direction: ltr;\n",
       "}\n",
       ".atoms {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".satom,.catom,.patom {\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    border-radius: 0.3em;\n",
       "    border-style: solid;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".sentence {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".clause {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".phrase {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".satom {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 4px;\n",
       "}\n",
       ".catom {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".patom {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".word {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 1px solid #cccccc;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".lextp {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 2px solid #888888;\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".occs {\n",
       "    font-size: x-small;\n",
       "}\n",
       ".satom.l,.catom.l,.patom.l {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".satom.r,.catom.r,.patom.r {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".satom.lno,.catom.lno,.patom.lno {\n",
       "    border-left-style: none\n",
       "}\n",
       ".satom.rno,.catom.rno,.patom.rno {\n",
       "    border-right-style: none\n",
       "}\n",
       ".tr,.tr a:visited,.tr a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".trb,.trb a:visited,.trb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: normal;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".prb,.prb a:visited,.prb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".h,.h a:visited,.h a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".hb,.hb a:visited,.hb a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    line-height: 2;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".vn {\n",
       "  font-size: small !important;\n",
       "  padding-right: 1em;\n",
       "}\n",
       ".rela,.function,.typ {\n",
       "    font-family: monospace;\n",
       "    font-size: small;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".pdp,.pdp a:visited,.pdp a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".voc_lex {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vs {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vt {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".gloss {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #444444;\n",
       "}\n",
       ".vrs {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: #444444;\n",
       "}\n",
       ".nd {\n",
       "    font-family: monospace;\n",
       "    font-size: x-small;\n",
       "    color: #999999;\n",
       "}\n",
       ".hl {\n",
       "    background-color: #ffee66;\n",
       "}\n",
       "span.hldot {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder: 0.2rem solid var(--hl-rim);\n",
       "\tborder-radius: 0.4rem;\n",
       "\t/*\n",
       "\tdisplay: inline-block;\n",
       "\twidth: 0.8rem;\n",
       "\theight: 0.8rem;\n",
       "\t*/\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "\n",
       "span.hlup {\n",
       "\tborder-color: var(--hl-dark);\n",
       "\tborder-width: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.2rem;\n",
       "  padding: 0.2rem;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55, 100%,  60%, 0.9  );\n",
       "\t--hl-dark:          hsla( 55, 100%,  40%, 0.9  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<details open><summary><b>API members</b>:</summary>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Computed/#computed-data\" title=\"doc\">C Computed</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Computed/#computed-data\" title=\"doc\">Call AllComputeds</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Computed/#computed-data\" title=\"doc\">Cs ComputedString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#edge-features\" title=\"doc\">E Edge</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#edge-features\" title=\"doc\">Eall AllEdges</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#edge-features\" title=\"doc\">Es EdgeString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">ensureLoaded</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">TF</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">ignored</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Fabric/#loading\" title=\"doc\">loadLog</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Locality/#locality\" title=\"doc\">L Locality</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">cache</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">error</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">indent</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">info</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Misc/#messaging\" title=\"doc\">reset</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">N Nodes</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">sortKey</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">sortKeyTuple</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">otypeRank</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Nodes/#navigating-nodes\" title=\"doc\">sortNodes</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#node-features\" title=\"doc\">F Feature</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#node-features\" title=\"doc\">Fall AllFeatures</a>, <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Features/#node-features\" title=\"doc\">Fs FeatureString</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Search/#search\" title=\"doc\">S Search</a><br/>\n",
       "<a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/Api/Text/#text\" title=\"doc\">T Text</a></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tf.app import use\n",
    "A = use('bhsa', hoist=globals())\n",
    "A.displaySetup(extraFeatures='g_cons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_books = ['Genesis', 'Exodus', 'Leviticus', 'Numbers', 'Deuteronomy', 'Joshua', 'Judges', '1_Samuel', '2_Samuel', \n",
    "               '1_Kings', '2_Kings', 'Isaiah', 'Jeremiah', 'Ezekiel', 'Hosea', 'Joel', 'Amos', 'Obadiah', 'Micah', \n",
    "               'Nahum', 'Habakkuk', 'Zephaniah', 'Haggai', 'Zechariah', 'Malachi', 'Psalms', 'Job', 'Proverbs', 'Ruth', \n",
    "               'Song_of_songs', 'Ecclesiastes', 'Lamentations', 'Esther', 'Daniel', 'Ezra', 'Nehemiah', \n",
    "               '1_Chronicles', '2_Chronicles']\n",
    "\n",
    "test_books = ['Jonah']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(books):\n",
    "\n",
    "    input_clauses = []\n",
    "    output_pos = []\n",
    "    input_chars = set()\n",
    "    output_vocab = set()\n",
    "    #nb_samples = 10000\n",
    "\n",
    "    # Process english and french sentences\n",
    "    #for line in range(nb_samples):\n",
    "    for cl in F.otype.s(\"clause\"): \n",
    "        \n",
    "        bo, _, _ = T.sectionFromNode(cl)\n",
    "        if bo not in books:\n",
    "            continue\n",
    "        \n",
    "        if len(L.d(cl, \"word\")) >7:\n",
    "            continue\n",
    "        #eng_line = str(lines[line]).split('\\t')[0]\n",
    "        words = \" \".join([F.g_cons.v(w) for w in L.d(cl, \"word\")])\n",
    "        pos_prepare = [F.sp.v(w) for w in L.d(cl, \"word\")]\n",
    "        poss = ['\\t']\n",
    "        for elem in pos_prepare:\n",
    "            poss.append(elem)\n",
    "        poss.append('\\n')\n",
    "    \n",
    "        # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
    "        #fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
    "        input_clauses.append(words)\n",
    "        output_pos.append(poss)\n",
    "    \n",
    "        for ch in words:\n",
    "            if (ch not in input_chars):\n",
    "                input_chars.add(ch)\n",
    "            \n",
    "        for ch in poss:\n",
    "            if (ch not in output_vocab):\n",
    "                output_vocab.add(ch)\n",
    "                \n",
    "    output_vocab = sorted(list(output_vocab))\n",
    "    input_chars = sorted(list(input_chars))\n",
    "    \n",
    "    max_len_input = max([len(line) for line in input_clauses])\n",
    "    max_len_output = max([len(line) for line in output_pos])\n",
    "    \n",
    "    return input_clauses, output_pos, input_chars, output_vocab, max_len_input, max_len_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test_data(books):\n",
    "\n",
    "    input_clauses = []\n",
    "    #output_pos = []\n",
    "    #input_chars = set()\n",
    "    #output_vocab = set()\n",
    "    #nb_samples = 10000\n",
    "\n",
    "    # Process english and french sentences\n",
    "    #for line in range(nb_samples):\n",
    "    for cl in F.otype.s(\"clause\"): \n",
    "        \n",
    "        bo, _, _ = T.sectionFromNode(cl)\n",
    "        if bo not in books:\n",
    "            continue\n",
    "        \n",
    "        if len(L.d(cl, \"word\")) > 7:\n",
    "            continue\n",
    "        #eng_line = str(lines[line]).split('\\t')[0]\n",
    "        words = \" \".join([F.g_cons.v(w) for w in L.d(cl, \"word\")])\n",
    "        #pos_prepare = [F.sp.v(w) for w in L.d(cl, \"word\")]\n",
    "        #poss = ['\\t']\n",
    "        #for elem in pos_prepare:\n",
    "        #    poss.append(elem)\n",
    "        #poss.append('\\n')\n",
    "    \n",
    "        # Append '\\t' for start of the sentence and '\\n' to signify end of the sentence\n",
    "        #fra_line = '\\t' + str(lines[line]).split('\\t')[1] + '\\n'\n",
    "        input_clauses.append(words)\n",
    "        #output_pos.append(poss)\n",
    "    \n",
    "        #f#or ch in words:\n",
    "        #    if (ch not in input_chars):\n",
    "        #        input_chars.add(ch)\n",
    "            \n",
    "        #for ch in poss:\n",
    "        #    if (ch not in output_vocab):\n",
    "        #        output_vocab.add(ch)\n",
    "                \n",
    "    #output_vocab = sorted(list(output_vocab))\n",
    "    #input_chars = sorted(list(input_chars))\n",
    "    \n",
    "    #max_len_input = max([len(line) for line in input_clauses])\n",
    "    #max_len_output = max([len(line) for line in output_pos])\n",
    "    \n",
    "    return input_clauses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dicts(input_chars, output_vocab):\n",
    "\n",
    "    # dictionary to index each english character - key is index and value is english character\n",
    "    eng_index_to_char_dict = {}\n",
    "\n",
    "    # dictionary to get english character given its index - key is english character and value is index\n",
    "    eng_char_to_index_dict = {}\n",
    "\n",
    "    for k, v in enumerate(input_chars):\n",
    "        eng_index_to_char_dict[k] = v\n",
    "        eng_char_to_index_dict[v] = k\n",
    "        \n",
    "    # dictionary to index each french character - key is index and value is french character\n",
    "    fra_index_to_char_dict = {}\n",
    "\n",
    "    # dictionary to get french character given its index - key is french character and value is index\n",
    "    fra_char_to_index_dict = {}\n",
    "    for k, v in enumerate(output_vocab):\n",
    "        fra_index_to_char_dict[k] = v\n",
    "        fra_char_to_index_dict[v] = k\n",
    "        \n",
    "    return eng_index_to_char_dict, eng_char_to_index_dict, fra_index_to_char_dict, fra_char_to_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(nb_samples, max_len_input, max_len_output, input_chars, output_vocab, eng_char_to_index_dict, fra_char_to_index_dict, input_clauses, output_pos):\n",
    "    tokenized_input_data = np.zeros(shape = (nb_samples,max_len_input,len(input_chars)), dtype='float32')\n",
    "    tokenized_output = np.zeros(shape = (nb_samples,max_len_output,len(output_vocab)), dtype='float32')\n",
    "    target_data = np.zeros((nb_samples, max_len_output, len(output_vocab)),dtype='float32')\n",
    "    # Vectorize the english and french sentences\n",
    "\n",
    "    for i in range(nb_samples):\n",
    "        for k,ch in enumerate(input_clauses[i]):\n",
    "            tokenized_input_data[i,k,eng_char_to_index_dict[ch]] = 1\n",
    "        \n",
    "        for k,ch in enumerate(output_pos[i]):\n",
    "            tokenized_output[i,k,fra_char_to_index_dict[ch]] = 1\n",
    "\n",
    "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "            if k > 0:\n",
    "                target_data[i,k-1,fra_char_to_index_dict[ch]] = 1\n",
    "                \n",
    "    return tokenized_input_data, tokenized_output, target_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_samples = 10000\n",
    "\n",
    "input_clauses, output_pos, input_chars, output_vocab, max_len_input, max_len_output = prepare_train_data(train_books)\n",
    "eng_index_to_char_dict, eng_char_to_index_dict, fra_index_to_char_dict, fra_char_to_index_dict = create_dicts(input_chars, output_vocab)\n",
    "tokenized_input, tokenized_output, target_data = one_hot_encode(nb_samples, max_len_input, max_len_output, input_chars, output_vocab, eng_char_to_index_dict, fra_char_to_index_dict, input_clauses, output_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_clauses = prepare_test_data(test_books)\n",
    "tokenized_test_data, _, _ = one_hot_encode(len(test_clauses), max_len_input, max_len_output, input_chars, output_vocab, eng_char_to_index_dict, fra_char_to_index_dict, test_clauses, output_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_LSTM_model(input_chars, output_vocab):\n",
    "\n",
    "    # Encoder model\n",
    "\n",
    "    encoder_input = Input(shape=(None,len(input_chars)))\n",
    "    encoder_LSTM = LSTM(512,activation = 'relu',return_state = True, return_sequences=True)(encoder_input)\n",
    "    encoder_LSTM = LSTM(512,return_state = True)(encoder_LSTM)\n",
    "    encoder_outputs, encoder_h, encoder_c = encoder_LSTM\n",
    "    encoder_states = [encoder_h, encoder_c]\n",
    "    \n",
    "    # Decoder model\n",
    "\n",
    "    decoder_input = Input(shape=(None,len(output_vocab)))\n",
    "    decoder_LSTM = LSTM(512, return_sequences=True, return_state = True)\n",
    "    decoder_out, _ , _ = decoder_LSTM(decoder_input, initial_state=encoder_states)\n",
    "    decoder_dense = Dense(len(output_vocab), activation='softmax')\n",
    "    decoder_out = decoder_dense (decoder_out)\n",
    "    \n",
    "    model = Model(inputs=[encoder_input, decoder_input],outputs=[decoder_out])\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_train(model, tokenized_input, tokenized_output, batch_size, epochs, validation_split):\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
    "    model.fit(x=[tokenized_input,tokenized_output], \n",
    "              y=target_data,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_split=validation_split)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_13 (InputLayer)           (None, None, 25)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_19 (LSTM)                  [(None, None, 512),  1101824     input_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_14 (InputLayer)           (None, None, 16)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_20 (LSTM)                  [(None, 512), (None, 2099200     lstm_19[0][0]                    \n",
      "                                                                 lstm_19[0][1]                    \n",
      "                                                                 lstm_19[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  [(None, None, 512),  1083392     input_14[0][0]                   \n",
      "                                                                 lstm_20[0][1]                    \n",
      "                                                                 lstm_20[0][2]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, None, 16)     8208        lstm_21[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,292,624\n",
      "Trainable params: 4,292,624\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "9000/9000 [==============================] - 18s 2ms/step - loss: 0.9517 - val_loss: 0.9256\n",
      "Epoch 2/5\n",
      "9000/9000 [==============================] - 8s 885us/step - loss: 0.8438 - val_loss: 0.8566\n",
      "Epoch 3/5\n",
      "9000/9000 [==============================] - 8s 903us/step - loss: 0.7951 - val_loss: 0.7398\n",
      "Epoch 4/5\n",
      "9000/9000 [==============================] - 8s 913us/step - loss: 0.6621 - val_loss: 0.6432\n",
      "Epoch 5/5\n",
      "9000/9000 [==============================] - 8s 908us/step - loss: 0.6302 - val_loss: 0.6390\n"
     ]
    }
   ],
   "source": [
    "model = define_LSTM_model(input_chars, output_vocab)\n",
    "model = compile_and_train(model, tokenized_input, tokenized_output, 128, 5, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference models for testing\n",
    "\n",
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_input, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "decoder_state_input_h = Input(shape=(512,))\n",
    "decoder_state_input_c = Input(shape=(512,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_LSTM(decoder_input, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_input] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_seq(inp_seq):\n",
    "    \n",
    "    # Initial states value is coming from the encoder \n",
    "    states_val = encoder_model_inf.predict(inp_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, len(output_vocab)))\n",
    "    target_seq[0, 0, fra_char_to_index_dict['\\t']] = 1\n",
    "    \n",
    "    translated_sent = ''\n",
    "    stop_condition = False\n",
    "    \n",
    "    while not stop_condition:\n",
    "        \n",
    "        decoder_out, decoder_h, decoder_c = decoder_model_inf.predict(x=[target_seq] + states_val)\n",
    "        \n",
    "        max_val_index = np.argmax(decoder_out[0,-1,:])\n",
    "        sampled_fra_char = fra_index_to_char_dict[max_val_index]\n",
    "        translated_sent += sampled_fra_char\n",
    "        \n",
    "        if (sampled_fra_char == '\\n'): #or (len(translated_sent) > max_len_fra_sent)) :\n",
    "            stop_condition = True\n",
    "        \n",
    "        target_seq = np.zeros((1, 1, len(output_vocab)))\n",
    "        target_seq[0, 0, max_val_index] = 1\n",
    "        \n",
    "        states_val = [decoder_h, decoder_c]\n",
    "        \n",
    "    return translated_sent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: L >MR\n",
      "Decoded sentence: prepverb\n",
      "\n",
      "-\n",
      "Input sentence: QWM\n",
      "Decoded sentence: verb\n",
      "\n",
      "-\n",
      "Input sentence: LK >L NJNWH H <JR H GDWLH\n",
      "Decoded sentence: verbprepsubsartsubsartprde\n",
      "\n",
      "-\n",
      "Input sentence: W QR> <LJH\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: KJ <LTH R<TM L PNJ\n",
      "Decoded sentence: conjverbsubsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: W JQM JWNH\n",
      "Decoded sentence: conjverbnmpr\n",
      "\n",
      "-\n",
      "Input sentence: L BRX TRCJCH M L PNJ JHWH\n",
      "Decoded sentence: prepverbsubsprepsubsartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W JRD JPW\n",
      "Decoded sentence: conjverbsubs\n",
      "\n",
      "-\n",
      "Input sentence: W JMY> >NJH\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: B>H TRCJC\n",
      "Decoded sentence: verbnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W JTN FKRH\n",
      "Decoded sentence: conjverbnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W JRD BH\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: W JHJ S<R GDWL B  JM\n",
      "Decoded sentence: conjverbsubsprepsubsartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W H >NJH XCBH\n",
      "Decoded sentence: conjartsubsverb\n",
      "\n",
      "-\n",
      "Input sentence: L HCBR\n",
      "Decoded sentence: prepverb\n",
      "\n",
      "-\n",
      "Input sentence: W JJR>W H MLXJM\n",
      "Decoded sentence: conjverbartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W JZ<QW\n",
      "Decoded sentence: conjverb\n",
      "\n",
      "-\n",
      "Input sentence: >JC >L >LHJW\n",
      "Decoded sentence: subsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: >CR B  >NJH\n",
      "Decoded sentence: conjprepartsubs\n",
      "\n",
      "-\n",
      "Input sentence: L HQL M <LJHM\n",
      "Decoded sentence: prepverbprepprep\n",
      "\n",
      "-\n",
      "Input sentence: W JWNH JRD >L JRKTJ H SPJNH\n",
      "Decoded sentence: conjverbnmprprepsubsartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W JCKB\n",
      "Decoded sentence: conjverb\n",
      "\n",
      "-\n",
      "Input sentence: W JRDM\n",
      "Decoded sentence: conjverb\n",
      "\n",
      "-\n",
      "Input sentence: W JQRB >LJW RB H XBL\n",
      "Decoded sentence: conjverbprepsubsartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W J>MR LW\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: MH LK\n",
      "Decoded sentence: prinprep\n",
      "\n",
      "-\n",
      "Input sentence: NRDM\n",
      "Decoded sentence: verb\n",
      "\n",
      "-\n",
      "Input sentence: QWM\n",
      "Decoded sentence: verb\n",
      "\n",
      "-\n",
      "Input sentence: QR> >L >LHJK\n",
      "Decoded sentence: verbprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: >WLJ JT<CT H >LHJM LNW\n",
      "Decoded sentence: advbverbprepartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W L> N>BD\n",
      "Decoded sentence: conjnegaverb\n",
      "\n",
      "-\n",
      "Input sentence: W J>MRW\n",
      "Decoded sentence: conjverb\n",
      "\n",
      "-\n",
      "Input sentence: >JC >L R<HW\n",
      "Decoded sentence: subsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: LKW\n",
      "Decoded sentence: verb\n",
      "\n",
      "-\n",
      "Input sentence: W NPJLH GWRLWT\n",
      "Decoded sentence: conjverbnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W ND<H\n",
      "Decoded sentence: conjverb\n",
      "\n",
      "-\n",
      "Input sentence: W JPLW GWRLWT\n",
      "Decoded sentence: conjverbnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W JPL H GWRL <L JWNH\n",
      "Decoded sentence: conjverbartsubsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: W J>MRW >LJW\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: HGJDH N> LNW\n",
      "Decoded sentence: verbintjprep\n",
      "\n",
      "-\n",
      "Input sentence: MH ML>KTK\n",
      "Decoded sentence: prinsubs\n",
      "\n",
      "-\n",
      "Input sentence: W M >JN TBW>\n",
      "Decoded sentence: conjprepsubsverb\n",
      "\n",
      "-\n",
      "Input sentence: MH >RYK\n",
      "Decoded sentence: prinsubs\n",
      "\n",
      "-\n",
      "Input sentence: W >J M ZH <M >TH\n",
      "Decoded sentence: conjconjprepprepartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W J>MR >LJHM\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: <BRJ >NKJ\n",
      "Decoded sentence: subsprps\n",
      "\n",
      "-\n",
      "Input sentence: W JJR>W H >NCJM JR>H GDWLH\n",
      "Decoded sentence: conjverbartsubsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: W J>MRW >LJW\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: MH Z>T <FJT\n",
      "Decoded sentence: prinprdeverb\n",
      "\n",
      "-\n",
      "Input sentence: KJ JD<W H >NCJM\n",
      "Decoded sentence: conjverbartsubs\n",
      "\n",
      "-\n",
      "Input sentence: KJ M L PNJ JHWH HW> BRX\n",
      "Decoded sentence: conjprepconjprepsubsartsubs\n",
      "\n",
      "-\n",
      "Input sentence: KJ HGJD LHM\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: W J>MRW >LJW\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: MH N<FH LK\n",
      "Decoded sentence: prinverbprep\n",
      "\n",
      "-\n",
      "Input sentence: W JCTQ H JM M <LJNW\n",
      "Decoded sentence: conjverbartsubsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: KJ H JM HWLK\n",
      "Decoded sentence: conjartsubsverb\n",
      "\n",
      "-\n",
      "Input sentence: W S<R\n",
      "Decoded sentence: conjverb\n",
      "\n",
      "-\n",
      "Input sentence: W J>MR >LJHM\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: F>WNJ\n",
      "Decoded sentence: verb\n",
      "\n",
      "-\n",
      "Input sentence: W HVJLNJ >L H JM\n",
      "Decoded sentence: conjverbprepartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W JCTQ H JM M <LJKM\n",
      "Decoded sentence: conjverbartsubsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: KJ JWD< >NJ\n",
      "Decoded sentence: conjadjvprps\n",
      "\n",
      "-\n",
      "Input sentence: W JXTRW H >NCJM\n",
      "Decoded sentence: conjverbartsubs\n",
      "\n",
      "-\n",
      "Input sentence: L HCJB >L H JBCH\n",
      "Decoded sentence: prepverbprepartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W L> JKLW\n",
      "Decoded sentence: conjnegaverb\n",
      "\n",
      "-\n",
      "Input sentence: KJ H JM HWLK\n",
      "Decoded sentence: conjartsubsverb\n",
      "\n",
      "-\n",
      "Input sentence: W S<R <LJHM\n",
      "Decoded sentence: conjverbprep\n",
      "\n",
      "-\n",
      "Input sentence: W JQR>W >L JHWH\n",
      "Decoded sentence: conjverbprepnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W J>MRW\n",
      "Decoded sentence: conjverb\n",
      "\n",
      "-\n",
      "Input sentence: >NH JHWH\n",
      "Decoded sentence: prpsnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W >L TTN <LJNW DM NQJ>\n",
      "Decoded sentence: conjnegaverbprepsubssubs\n",
      "\n",
      "-\n",
      "Input sentence: KJ >TH <FJT\n",
      "Decoded sentence: conjadvbverb\n",
      "\n",
      "-\n",
      "Input sentence: JHWH\n",
      "Decoded sentence: nmpr\n",
      "\n",
      "-\n",
      "Input sentence: K >CR XPYT\n",
      "Decoded sentence: prepconjverb\n",
      "\n",
      "-\n",
      "Input sentence: W JF>W >T JWNH\n",
      "Decoded sentence: conjverbprepnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W JVLHW >L H JM\n",
      "Decoded sentence: conjverbprepartsubs\n",
      "\n",
      "-\n",
      "Input sentence: W J<MD H JM M Z<PW\n",
      "Decoded sentence: conjverbartsubsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: W JZBXW ZBX L JHWH\n",
      "Decoded sentence: conjverbnmprprepnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W JDRW NDRJM\n",
      "Decoded sentence: conjverbsubs\n",
      "\n",
      "-\n",
      "Input sentence: W JMN JHWH DG GDWL\n",
      "Decoded sentence: conjverbnmprsubsnmpr\n",
      "\n",
      "-\n",
      "Input sentence: L BL< >T JWNH\n",
      "Decoded sentence: prepverbprepnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W J>MR\n",
      "Decoded sentence: conjverb\n",
      "\n",
      "-\n",
      "Input sentence: QR>TJ M YRH LJ >L JHWH\n",
      "Decoded sentence: verbprepsubsprepsubsnmpr\n",
      "\n",
      "-\n",
      "Input sentence: W J<NNJ\n",
      "Decoded sentence: conjverb\n",
      "\n",
      "-\n",
      "Input sentence: M BVN C>WL CW<TJ\n",
      "Decoded sentence: prepsubsverbsubs\n",
      "\n",
      "-\n",
      "Input sentence: CM<T QWLJ\n",
      "Decoded sentence: verbsubs\n",
      "\n",
      "-\n",
      "Input sentence: W TCLJKNJ MYWLH B LBB JMJM\n",
      "Decoded sentence: conjverbsubsprepnmprsubs\n",
      "\n",
      "-\n",
      "Input sentence: W NHR JSBBNJ\n",
      "Decoded sentence: conjverbnmpr\n",
      "\n",
      "-\n",
      "Input sentence: KL MCBRJK W GLJK <LJ <BRW\n",
      "Decoded sentence: subssubssubsconjsubssubs\n",
      "\n",
      "-\n",
      "Input sentence: W >NJ >MRTJ\n",
      "Decoded sentence: conjprpssubs\n",
      "\n",
      "-\n",
      "Input sentence: NGRCTJ M NGD <JNJK\n",
      "Decoded sentence: verbprepsubssubs\n",
      "\n",
      "-\n",
      "Input sentence: >K >WSJP\n",
      "Decoded sentence: advbverb\n",
      "\n",
      "-\n",
      "Input sentence: L HBJV >L HJKL QDCK\n",
      "Decoded sentence: prepverbprepsubsnmpr\n",
      "\n",
      "-\n",
      "Input sentence: >PPWNJ MJM <D NPC\n",
      "Decoded sentence: verbsubsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: THWM JSBBNJ\n",
      "Decoded sentence: verbnmpr\n",
      "\n",
      "-\n",
      "Input sentence: SWP XBWC L R>CJ\n",
      "Decoded sentence: verbsubsprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: L QYBJ HRJM JRDTJ\n",
      "Decoded sentence: prepverbsubssubs\n",
      "\n",
      "-\n",
      "Input sentence: H >RY\n",
      "Decoded sentence: artsubs\n",
      "\n",
      "-\n",
      "Input sentence: BRXJH B<DJ L <WLM\n",
      "Decoded sentence: subsverbprepsubs\n",
      "\n",
      "-\n",
      "Input sentence: W T<L M CXT XJJ\n",
      "Decoded sentence: conjverbprepsubssubs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(100):\n",
    "    inp_seq = tokenized_test_data[seq_index:seq_index+1]\n",
    "    \n",
    "    translated_sent = decode_seq(inp_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', test_clauses[seq_index])\n",
    "    print('Decoded sentence:', translated_sent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
